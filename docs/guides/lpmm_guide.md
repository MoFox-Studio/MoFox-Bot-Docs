# 📄 麦麦知识库（LPMM）使用指南

## 简介

欢迎使用全新升级的麦麦知识库（LPMM）！我们引入了一款强大的一体化学习工具，它将以往复杂的多步操作整合进一个简单的交互式菜单。现在，为你的 MoFox 注入知识，就像与一位智能助手对话一样轻松自然。

新版 LPMM 的核心优势：
- **一键启动**：告别繁琐的命令，一个脚本就能搞定一切。
- **交互式菜单**：清晰的选项引导，让你准确知道每一步在做什么。
- **自动化流程**：从数据清洗、信息提取到最终导入，全程自动化，省时省力。
- **零配置**：不再需要复杂的配置文件，开箱即用。

## 注意事项
:::warning
请仔细阅读以下注意事项，以免引起不必要的麻烦与支出
:::

在开始之前，请花几分钟阅读以下提示，这将帮助你获得最佳的知识学习效果，并避免不必要的开销：

*   **文本质量**：知识源的质量是关键。请确保你的 `.txt` 文件分段清晰，没有无关的符号或格式，这将直接影响 AI 的理解和提取效果。
*   **模型选择与费用**：知识提取是一个计算密集型任务。我们推荐使用性能均衡的大模型（如 32B 至 72B 级别），以在效果和成本之间找到最佳平衡点。
*   **API 速率**：知识学习过程会向模型 API 发起大量请求。请留意你所选模型服务的速率限制，避免因请求过快而被暂时封禁。
*   **系统资源**：在进行知识导入时，脚本会大量占用 CPU 和内存资源。建议在性能较好的计算机上运行此工具。

## 快速上手：三步构建你的专属知识库

### 第一步：准备你的“教材”

首先，你需要为你的人工智能准备学习材料。

1.  **创建目录**：在项目根目录的 `data` 文件夹下，创建一个名为 `lpmm_raw_data` 的新文件夹。
2.  **准备文件**：将你的知识源文件整理成一个或多个 `.txt` 文件。
3.  **内容格式**：
    *   **一个主题一段落**：将相关联的内容组织在同一个段落中。
    *   **空行分隔**：使用一个空行来分隔不同的段落。这能帮助 AI 更好地理解上下文。

    **优秀示例**：
    ```
    精神状态良好：这是一种网络流行语，通常用来反讽自己实际上精神状态非常不稳定，是年轻人在高压生活下的一种幽默自嘲。

    躺平：指无论外界如何变化，内心都毫无波澜，不再进行无谓的挣扎和反抗。它体现了一种顺从和“佛系”的心态，表示对激烈竞争的厌倦。
    ```
4.  **放入文件**：将准备好的 `.txt` 文件全部放入 `data/lpmm_raw_data` 文件夹中。

### 第二步：启动“智能学习助手”

现在，让我们唤醒这位智能学习助手。

1.  **激活虚拟环境**：打开终端，并激活你的 Python 虚拟环境。
    ```bash
    # 如果你在 Windows 上
    .\venv\Scripts\activate

    # 如果你在 Linux 或 macOS 上
    source ./venv/bin/activate
    ```
2.  **运行学习工具**：执行以下命令，启动 LPMM 学习工具。
    ```bash
    cd MoFox-Bot #CD到你的机器人目录下面
    python ./scripts/lpmm_learning_tool.py
    #uv用户使用这个
    uv python ./scripts/lpmm_learning_tool.py
    ```

### 第三步：选择学习模式

运行脚本后，你会看到一个清晰的功能菜单。这就像是给你的 AI 助手下达指令：

```
=== LPMM 知识库学习工具 ===
1. [数据预处理] -> 读取 .txt 文件 (来源: ./mmc/data/lpmm_raw_data/)
2. [信息提取] -> 提取信息并存为 .json (输出至: ./mmc/data/openie/)
3. [数据导入] -> 从 openie 文件夹自动导入最新知识
4. [全流程] -> 按顺序执行 1 -> 2 -> 3
5. [指定导入] -> 从特定的 openie.json 文件导入知识
0. [退出]
------------------------------
请输入你的选择 (0-5):
```

**菜单功能导览**：

*   **选项 1: [数据预处理]**
    *   **它做什么？** 帮你通读并整理一遍“教材”，去除所有重复的内容。
    *   **何时使用？** 如果你只想检查一下数据，或者分步执行，可以先选择它。

*   **选项 2: [信息提取]**
    *   **它做什么？** 启动 AI 进行深度阅读，从你的文本中划出所有的“知识点”（即实体和它们之间的关系）。
    *   **何时使用？** 当你完成了数据预处理，或者想单独进行信息提取时。

*   **选项 3: [数据导入]**
    *   **它做什么？** 将所有划好的“知识点”正式、永久地记入 AI 的“大脑”（知识图谱和向量数据库）中。
    *   **何时使用？** 当你已经生成了知识点文件（.json），并希望将其导入系统时。

*   **选项 4: [全流程]**
    *   **它做什么？** **一键托管模式！** 它会自动按顺序完成“整理教材 -> 划重点 -> 记入大脑”的全过程。
    *   **何时使用？** **强烈推荐初次使用者或希望一次性完成所有操作的用户选择此项。**

*   **选项 5: [指定导入]**
    *   **它做什么？** 如果你手头有一份之前提取好的“知识点笔记”（特定的 `openie.json` 文件），这个选项可以让你直接将其导入。
    *   **何时使用？** 用于数据迁移或从特定备份文件恢复。

## 进阶技巧

### GPU 加速

如果你的电脑配备了 NVIDIA 显卡（RTX 20系及以上，仅限 Linux），可以通过安装 GPU 版本的 `faiss` 库来大幅提升知识导入的速度。

```bash
# 首先，卸载可能已存在的 CPU 版本
pip uninstall faiss-cpu

# 然后，根据你的 CUDA 版本选择安装
# CUDA 11.x
pip install faiss-gpu-cu11
# CUDA 12.x
pip install faiss-gpu-cu12
```

## 疑难解答 (FAQ)

**问：我运行脚本时失败了，好像提示和 `quick_algo` 有关的错误？**

答：`quick_algo` 是一个用于加速某些算法的依赖库。在大多数情况下，你不需要手动处理它。但如果脚本确实因为缺少它而失败，你可以按照以下步骤手动安装：

*   **对于 Windows (x86_64)**:
    ```bash
    pip install quick_algo
    ```

*   **对于 Linux**:
    1.  确保你安装了 C++ 编译器（GCC/G++）：
        ```bash
        # Debian/Ubuntu
        sudo apt update && sudo apt install build-essential
        # Red Hat/Fedora/CentOS
        sudo dnf install gcc gcc-c++
        ```
    2.  然后，在你的虚拟环境中安装：
        ```bash
        pip install quick-algo
        ```

*   **对于 macOS**:
    请参考 [MaiCore官方的手动编译文档](https://docs.mai-mai.org/manual/usage/compile_and_install.html) 进行安装。